{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","authorship_tag":"ABX9TyNdnqzXqP+eYLi7h/Q6bqO9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EwuOrUuJZwgL"},"outputs":[],"source":["#!pip install bertopic\n","#!pip install sentence-transformers\n","#!pip install umap-learn\n","#!pip install hdbscan\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","\n","# Load a multilingual embedding model\n","embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n","\n","# Initialize BERTopic with the model\n","topic_model = BERTopic(embedding_model=embedding_model, verbose=True)\n","\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","import pandas as pd\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/My Drive/mergeddf.csv\")\n","\n","# Store original indices to ensure proper alignment later\n","original_indices = df.index[df[\"rawtext\"].notna()]\n","\n","# Select non-NaN texts\n","texts = df.loc[original_indices, \"rawtext\"].tolist()\n","\n","# Fit the BERTopic model\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Get full topic probability distributions\n","topic_distributions = topic_model.transform(texts)[1]\n","\n","# Convert topic distributions to DataFrame\n","topic_probs_df = pd.DataFrame(topic_distributions, columns=[f\"topic_{i}\" for i in range(topic_distributions.shape[1])])\n","\n","# Assign the original indices to the topic_probs_df\n","topic_probs_df.index = original_indices\n","\n","# Merge topic probabilities back to the original DataFrame\n","df = pd.concat([df, topic_probs_df], axis=1)\n","\n","# Save the merged DataFrame with topic probabilities\n","df.to_csv(\"/content/drive/My Drive/general_merged_with_topics.csv\", index=False)\n","\n","# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/topic_model_general1\")\n","\n","print(\"Topic probabilities merged and saved correctly with original alignment.\")\n","\n","topic_model.visualize_barchart()\n","print(topic_model.visualize_barchart)\n","topic_model.get_topic_freq()\n","print(topic_model.get_topic_freq)\n","topic_model.visualize_barchart()\n","print(topic_model.visualize_barchart)\n","topic_model.get_topics()\n","print(topic_model.get_topics)\n","topic_model.get_topic_info()\n","print(topic_model.get_topic_info)\n","topic_model.visualize_topics()\n","print(topic_model.visualize_topics)\n","\n"]},{"cell_type":"code","source":["print(type(topic_distributions))\n","print(topic_distributions)\n","print(topic_distributions.shape if hasattr(topic_distributions, 'shape') else \"No shape attribute\")\n","\n","topic_distributions = topic_model.transform(texts)[1]\n","topic_distributions = [topic_distribution if isinstance(topic_distribution, (np.ndarray, list)) else [topic_distribution] for topic_distribution in topic_distributions]\n","import numpy as np\n","topic_distributions = np.array(topic_distributions)\n","\n","# Convert topic distributions to DataFrame\n","topic_probs_df = pd.DataFrame(topic_distributions, columns=[f\"topic_{i}\" for i in range(topic_distributions.shape[1])])"],"metadata":{"id":"bYnjlwaKMg5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np # Moved the import statement to before the list comprehension\n","topic_distributions = [topic_distribution if isinstance(topic_distribution, (np.ndarray, list)) else [topic_distribution] for topic_distribution in topic_distributions]\n","topic_distributions = np.array(topic_distributions)\n","\n","# Convert topic distributions to DataFrame\n","topic_probs_df = pd.DataFrame(topic_distributions, columns=[f\"topic_{i}\" for i in range(topic_distributions.shape[1])])"],"metadata":{"id":"SxkBR2xINPqK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(topic_probs_df.head(10))"],"metadata":{"id":"fNPlzTjsZgfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get topic information\n","topic_info_df = topic_model.get_topic_info()\n","\n","# Get the number of topics (excluding the -1 topic, which represents outliers)\n","num_topics = len(topic_info_df) - 1\n","\n","# Print the number of topics\n","print(f\"Number of topics in the dataset: {num_topics}\")"],"metadata":{"id":"lnXn7yU9Zqim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","\n","topic_probs_df = pd.DataFrame(topic_distributions, columns=[f\"topic_{i}\" for i in range(topic_distributions.shape[1])])\n","\n","# Assign the original indices to the topic_probs_df\n","topic_probs_df.index = original_indices\n","\n","# Merge topic probabilities back to the original DataFrame\n","df = pd.concat([df, topic_probs_df], axis=1)\n","\n","# Get the dominant topic for each document\n","df['Dominant_Topic'] = topic_model.topics_  # Assuming topic_model.topics_ contains topic assignments for each document\n","\n","# Save the merged DataFrame with topic probabilities and dominant topic\n","df.to_csv(\"/content/drive/My Drive/merged_with_topics_and_dominant.csv\", index=False)\n","\n","print(\"Topic probabilities, dominant topic merged, and saved correctly with original alignment.\")\n","# Save the topic descriptions\n","topics_info.to_csv(\"/content/drive/My Drive/topic_descriptions2.csv\", index=False)\n","\n","# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/topic_model_general1\")"],"metadata":{"id":"_9mOYR2haI7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import numpy as np\n","from sklearn.metrics import silhouette_score  # Import silhouette_score\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","\n","# Calculate silhouette scores for a range of topic numbers\n","silhouette_scores = []\n","topic_numbers = range(64, 84, 1)  # Adjust range as needed\n","for nr_topics in topic_numbers:\n","    topic_model = BERTopic(embedding_model=embedding_model, verbose=True, nr_topics=nr_topics) # Re-fitting\n","    topic_model.fit_transform(texts)  # Re-fit the model\n","    embeddings = topic_model._extract_embeddings(texts, method=\"document\")\n","    labels = topic_model.hdbscan_model.labels_\n","    score = silhouette_score(embeddings, labels)\n","    silhouette_scores.append(score)\n","\n","# Find the optimal number of topics with the highest silhouette score\n","optimal_nr_topics = topic_numbers[np.argmax(silhouette_scores)]\n","\n","# Re-fit the model with the optimal number of topics\n","topic_model = BERTopic(embedding_model=embedding_model, verbose=True, nr_topics=optimal_nr_topics)\n","topic_model.fit_transform(texts)\n","\n","# Update the DataFrame with the new topic information\n","new_topics, new_probs = topic_model.transform(texts)\n","topic_probs_df = pd.DataFrame(new_probs, columns=[f\"topic_{i}\" for i in range(new_probs.shape[1])])\n","topic_probs_df.index = original_indices\n","df = pd.concat([df, topic_probs_df], axis=1)\n","df['Dominant_Topic'] = new_topics  # Update with new topic assignments\n","\n","# Save the updated DataFrame\n","df.to_csv(\"/content/drive/My Drive/merged_with_optimized_topics.csv\", index=False)\n","\n","topics_info.to_csv(\"/content/drive/My Drive/optimized_topic_descriptions2.csv\", index=False)\n","\n","# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_general1\")"],"metadata":{"id":"cR95lCr6a1AK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_new\")"],"metadata":{"id":"qqCGVQ-1h031"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Update the DataFrame with the new topic information\n","new_topics, new_probs = topic_model.transform(texts)\n","if new_probs is not None and isinstance(new_probs, np.ndarray) and new_probs.ndim == 2:\n","    topic_probs_df = pd.DataFrame(new_probs, columns=[f\"topic_{i}\" for i in range(new_probs.shape[1])])\n","    topic_probs_df.index = original_indices\n","    df = pd.concat([df, topic_probs_df], axis=1)\n","else:\n","    print(\"No new topic distributions found or the output format is unexpected.\")\n","df['Dominant_Topic'] = new_topics\n","\n","# Save the updated DataFrame\n","df.to_csv(\"/content/drive/My Drive/merged_with_optimized_topics.csv\", index=False)\n"],"metadata":{"id":"rFChop6yh8uH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Find the optimal number of topics with the highest silhouette score\n","optimal_nr_topics = topic_numbers[np.argmax(silhouette_scores)]\n","print(f\"The optimal number of topics is: {optimal_nr_topics}\")\n","\n","# Plot silhouette scores\n","plt.figure(figsize=(10, 6))\n","plt.plot(topic_numbers, silhouette_scores, marker='o')\n","plt.title('Silhouette Scores for Different Topic Numbers')\n","plt.xlabel('Number of Topics')\n","plt.ylabel('Silhouette Score')\n","plt.grid(True)\n","plt.savefig(\"/content/drive/My Drive/silhouette_score_plot.png\")\n","plt.show()"],"metadata":{"id":"QJAz6jDCL-bD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install bertopic sentence-transformers umap-learn hdbscan\n","\n","import pandas as pd\n","import numpy as np\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","from google.colab import drive\n","from sklearn.metrics import silhouette_score\n","import umap\n","from hdbscan import HDBSCAN\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/My Drive/mergeddf.csv\")\n","\n","# Store original indices to ensure proper alignment later\n","original_indices = df.index[df[\"rawtext\"].notna()]\n","\n","# Select non-NaN texts\n","texts = df.loc[original_indices, \"rawtext\"].tolist()\n","\n","# Load a multilingual embedding model\n","embedding_model = SentenceTransformer(\"xlm-r-100langs-bert-base-nli-stsb-mean-tokens\")\n","\n","# Initialize UMAP for broader, general topics\n","umap_model = umap.UMAP(n_neighbors=30, n_components=5, metric='cosine')\n","\n","# Initialize HDBSCAN to form fewer, larger clusters\n","hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples=10)\n","\n","# Initialize BERTopic with custom UMAP and HDBSCAN models\n","topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True)\n","\n","# Fit the BERTopic model and save progress\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Extract document embeddings\n","embeddings = topic_model._extract_embeddings(texts, method=\"document\")\n","\n","# Get HDBSCAN labels\n","labels = topic_model.hdbscan_model.labels_\n","\n","# Calculate silhouette score\n","if len(set(labels)) > 1 and -1 not in set(labels):\n","    score = silhouette_score(embeddings, labels)\n","    print(f\"Silhouette Score: {score}\")\n","else:\n","    print(\"Not enough distinct clusters to compute silhouette score.\")\n","\n","# Display topic descriptions\n","topics_info = topic_model.get_topic_info()\n","print(\"\\nTopic Descriptions:\")\n","print(topics_info[['Topic', 'Name']])\n","\n","# Save the topic descriptions\n","topics_info.to_csv(\"/content/drive/My Drive/topic_descriptions2.csv\", index=False)\n","\n","# Add topic probabilities to the DataFrame\n","if probs is not None and isinstance(probs, np.ndarray) and probs.ndim == 2:\n","    topic_probs_df = pd.DataFrame(probs, columns=[f\"topic_{i}\" for i in range(probs.shape[1])])\n","    topic_probs_df.index = original_indices\n","    df = pd.concat([df, topic_probs_df], axis=1)\n","else:\n","    print(\"No topic probabilities found or the output format is unexpected.\")\n","\n","# Add the dominant topic for each document\n","df['Dominant_Topic'] = topics\n","\n","# Save the updated DataFrame with topic probabilities\n","df.to_csv(\"/content/drive/My Drive/merged_with_topics_and_probs2.csv\", index=False)\n","\n","# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_general2\")\n"],"metadata":{"id":"ZESuRMofmps0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize UMAP for broader, general topics\n","umap_model = umap.UMAP(n_neighbors=40, n_components=5, metric='cosine')\n","\n","# Initialize HDBSCAN to form fewer, larger clusters\n","hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=8)\n","\n","# Initialize BERTopic with custom UMAP and HDBSCAN models\n","topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True)\n","\n","# Fit the BERTopic model and save progress\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Extract document embeddings\n","embeddings = topic_model._extract_embeddings(texts, method=\"document\")\n","\n","# Get HDBSCAN labels\n","labels = topic_model.hdbscan_model.labels_\n","\n","# Calculate silhouette score\n","if len(set(labels)) > 1 and -1 not in set(labels):\n","    score = silhouette_score(embeddings, labels)\n","    print(f\"Silhouette Score: {score}\")\n","else:\n","    print(\"Not enough distinct clusters to compute silhouette score.\")\n","\n","# Display topic descriptions\n","topics_info = topic_model.get_topic_info()\n","print(\"\\nTopic Descriptions:\")\n","print(topics_info[['Topic', 'Name']])\n","\n","# Save the topic descriptions\n","topics_info.to_csv(\"/content/drive/My Drive/topic_descriptions3.csv\", index=False)\n","\n","# Add topic probabilities to the DataFrame\n","if probs is not None and isinstance(probs, np.ndarray) and probs.ndim == 2:\n","    topic_probs_df = pd.DataFrame(probs, columns=[f\"topic_{i}\" for i in range(probs.shape[1])])\n","    topic_probs_df.index = original_indices\n","    df = pd.concat([df, topic_probs_df], axis=1)\n","else:\n","    print(\"No topic probabilities found or the output format is unexpected.\")\n","\n","# Add the dominant topic for each document\n","df['Dominant_Topic'] = topics\n","\n","# Save the updated DataFrame with topic probabilities\n","df.to_csv(\"/content/drive/My Drive/merged_with_topics_and_probs3.csv\", index=False)\n","\n","# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_general3\")\n"],"metadata":{"id":"FW96nqqxq5TO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize UMAP for broader, general topics\n","umap_model = umap.UMAP(n_neighbors=40, n_components=5, metric='cosine')\n","\n","# Initialize HDBSCAN to form fewer, larger clusters\n","hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=5)\n","\n","# Initialize BERTopic with custom UMAP and HDBSCAN models\n","topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True)\n","\n","# Fit the BERTopic model and save progress\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Extract document embeddings\n","embeddings = topic_model._extract_embeddings(texts, method=\"document\")\n","\n","# Get HDBSCAN labels\n","labels = topic_model.hdbscan_model.labels_\n","\n","# Calculate silhouette score\n","if len(set(labels)) > 1 and -1 not in set(labels):\n","    score = silhouette_score(embeddings, labels)\n","    print(f\"Silhouette Score: {score}\")\n","else:\n","    print(\"Not enough distinct clusters to compute silhouette score.\")\n","\n","# Display topic descriptions\n","topics_info = topic_model.get_topic_info()\n","print(\"\\nTopic Descriptions:\")\n","print(topics_info[['Topic', 'Name']])\n","\n","# Save the topic descriptions\n","topics_info.to_csv(\"/content/drive/My Drive/topic_descriptions4.csv\", index=False)\n","\n","# Add topic probabilities to the DataFrame\n","if probs is not None and isinstance(probs, np.ndarray) and probs.ndim == 2:\n","    topic_probs_df = pd.DataFrame(probs, columns=[f\"topic_{i}\" for i in range(probs.shape[1])])\n","    topic_probs_df.index = original_indices\n","    df = pd.concat([df, topic_probs_df], axis=1)\n","else:\n","    print(\"No topic probabilities found or the output format is unexpected.\")\n","\n","# Add the dominant topic for each document\n","df['Dominant_Topic'] = topics\n","\n","# Save the updated DataFrame with topic probabilities\n","df.to_csv(\"/content/drive/My Drive/merged_with_topics_and_probs4.csv\", index=False)\n","\n","# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_general4\")\n"],"metadata":{"id":"zNlJuBslxnj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize UMAP for broader, general topics\n","umap_model = umap.UMAP(n_neighbors=40, n_components=5, metric='cosine')\n","\n","# Initialize HDBSCAN to form fewer, larger clusters\n","hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples=5)\n","\n","# Initialize BERTopic with custom UMAP and HDBSCAN models\n","topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True)\n","\n","# Fit the BERTopic model and save progress\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Extract document embeddings\n","embeddings = topic_model._extract_embeddings(texts, method=\"document\")\n","\n","# Get HDBSCAN labels\n","labels = topic_model.hdbscan_model.labels_\n","\n","# Calculate silhouette score\n","if len(set(labels)) > 1 and -1 not in set(labels):\n","    score = silhouette_score(embeddings, labels)\n","    print(f\"Silhouette Score: {score}\")\n","else:\n","    print(\"Not enough distinct clusters to compute silhouette score.\")\n","\n","# Display topic descriptions\n","topics_info = topic_model.get_topic_info()\n","print(\"\\nTopic Descriptions:\")\n","print(topics_info[['Topic', 'Name']])\n","\n","# Save the topic descriptions\n","topics_info.to_csv(\"/content/drive/My Drive/topic_descriptions5.csv\", index=False)\n","\n","# Add topic probabilities to the DataFrame\n","if probs is not None and isinstance(probs, np.ndarray) and probs.ndim == 2:\n","    topic_probs_df = pd.DataFrame(probs, columns=[f\"topic_{i}\" for i in range(probs.shape[1])])\n","    topic_probs_df.index = original_indices\n","    df = pd.concat([df, topic_probs_df], axis=1)\n","else:\n","    print(\"No topic probabilities found or the output format is unexpected.\")\n","\n","# Add the dominant topic for each document\n","df['Dominant_Topic'] = topics\n","\n","# Save the updated DataFrame with topic probabilities\n","df.to_csv(\"/content/drive/My Drive/merged_with_topics_and_probs5.csv\", index=False)\n","\n","# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_general5\")\n"],"metadata":{"id":"Xec2noJ9xtMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install required libraries and language models\n","!pip install bertopic sentence-transformers umap-learn hdbscan\n","!python -m spacy download fr_core_news_sm\n","!python -m spacy download de_core_news_sm\n","\n","from bertopic import BERTopic\n","import pandas as pd\n","import spacy\n","\n","# Load English, French, and German NLP models for text processing\n","nlp_en = spacy.load(\"en_core_web_sm\")\n","nlp_fr = spacy.load(\"fr_core_news_sm\")\n","nlp_de = spacy.load(\"de_core_news_sm\")\n","\n","# Define stop words in English, French, and German\n","stop_words_en = {\"the\", \"and\", \"of\", \"in\", \"to\", \"a\", \"for\", \"on\", \"is\", \"with\", \"as\", \"that\", \"this\", \"was\", \"by\"}\n","stop_words_fr = {\"le\", \"la\", \"les\", \"et\", \"de\", \"du\", \"des\", \"en\", \"à\", \"un\", \"une\", \"pour\", \"avec\", \"est\", \"par\", \"ce\", \"cette\", \"ça\"}\n","stop_words_de = {\"der\", \"die\", \"das\", \"und\", \"in\", \"zu\", \"mit\", \"auf\", \"für\", \"ist\", \"von\", \"ein\", \"eine\", \"es\", \"an\", \"dem\", \"den\", \"diese\", \"dieser\"}\n","\n","# Combine stop words from all three languages\n","stop_words = stop_words_en | stop_words_fr | stop_words_de\n","\n","# Load the saved BERTopic model\n","topic_model = BERTopic.load(\"/content/drive/My Drive/optimized_topic_model_general3\")\n","\n","# Get topic descriptions\n","topics_info = topic_model.get_topic_info()\n","\n","# Function to clean topic descriptions\n","def clean_topic_words(topic_name):\n","    words = topic_name.split(\"_\")  # BERTopic separates words with \"_\"\n","\n","    # Process words in each language separately\n","    filtered_words = []\n","\n","    for word in words:\n","        # Process each word with all three language models\n","        for nlp_model in [nlp_en, nlp_fr, nlp_de]:\n","            doc = nlp_model(word)\n","            for token in doc:\n","                if token.pos_ == \"NOUN\" and len(token.text) > 4 and token.text.lower() not in stop_words:\n","                    filtered_words.append(token.text)\n","\n","    return \" \".join(set(filtered_words))  # Remove duplicates and join words\n","\n","# Apply cleaning to topic names\n","topics_info[\"Cleaned_Name\"] = topics_info[\"Name\"].apply(clean_topic_words)\n","\n","# Save cleaned topic descriptions\n","output_path = \"/content/drive/My Drive/topic_descriptions_cleaned.csv\"\n","topics_info.to_csv(output_path, index=False)\n","\n","print(f\"Cleaned topic descriptions saved to {output_path}\")\n"],"metadata":{"id":"ew5iydSvAPFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import bertopic\n","print(bertopic.__version__)\n","import bertopic\n","print(dir(bertopic))\n"],"metadata":{"id":"3XsTGc6ImZHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","from hdbscan import HDBSCAN\n","import umap\n","from bertopic.evaluation import coherence_score\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/My Drive/mergeddf.csv\")\n","\n","# Ensure 'rawtext' column exists\n","if \"rawtext\" not in df.columns:\n","    raise ValueError(\"Column 'rawtext' not found in dataset.\")\n","\n","# Filter out NaN texts\n","df = df[df[\"rawtext\"].notna()].reset_index(drop=True)\n","texts = df[\"rawtext\"].tolist()\n","\n","# Load embedding model\n","embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n","\n","# Define parameter ranges to test\n","min_cluster_sizes = range(20, 50, 10)  # Controls topic size\n","n_neighbors_values = range(10, 50, 10)  # Controls local/global structure\n","n_components_values = [3, 5, 7, 9]  # Controls dimensionality reduction\n","\n","# Store results\n","results = []\n","\n","# Iterate over all parameter combinations\n","for min_cluster_size in min_cluster_sizes:\n","    for n_neighbors in n_neighbors_values:\n","        for n_components in n_components_values:\n","            print(f\"Testing: min_cluster_size={min_cluster_size}, n_neighbors={n_neighbors}, n_components={n_components}\")\n","\n","            # Define UMAP and HDBSCAN models\n","            umap_model = umap.UMAP(n_neighbors=n_neighbors, n_components=n_components, metric='cosine')\n","            hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=5)\n","\n","            # Initialize BERTopic\n","            topic_model = BERTopic(\n","                embedding_model=embedding_model,\n","                umap_model=umap_model,\n","                hdbscan_model=hdbscan_model,\n","                calculate_probabilities=True,\n","                verbose=True\n","            )\n","\n","            # Fit the model\n","            topics, _ = topic_model.fit_transform(texts)\n","\n","            # Compute Coherence Score\n","            coherence = coherence_score(topic_model, texts)\n","\n","            # Save results\n","            results.append({\n","                \"min_cluster_size\": min_cluster_size,\n","                \"n_neighbors\": n_neighbors,\n","                \"n_components\": n_components,\n","                \"coherence_score\": coherence\n","            })\n","\n","# Convert results to DataFrame\n","results_df = pd.DataFrame(results)\n","\n","# Find best model based on Coherence Score\n","best_model = results_df.sort_values(by=\"coherence_score\", ascending=False).iloc[0]\n","best_min_cluster_size = best_model[\"min_cluster_size\"]\n","best_n_neighbors = best_model[\"n_neighbors\"]\n","best_n_components = best_model[\"n_components\"]\n","\n","print(f\"Best Parameters: min_cluster_size={best_min_cluster_size}, n_neighbors={best_n_neighbors}, n_components={best_n_components}\")\n","\n","# Re-run BERTopic with best parameters\n","optimal_umap = umap.UMAP(n_neighbors=best_n_neighbors, n_components=best_n_components, metric='cosine')\n","optimal_hdbscan = HDBSCAN(min_cluster_size=best_min_cluster_size, min_samples=5)\n","\n","topic_model = BERTopic(\n","    embedding_model=embedding_model,\n","    umap_model=optimal_umap,\n","    hdbscan_model=optimal_hdbscan,\n","    calculate_probabilities=True,\n","    verbose=True\n",")\n","\n","# Fit final model\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Save topic descriptions\n","topics_info = topic_model.get_topic_info()\n","topics_info.to_csv(\"/content/drive/My Drive/optimized_topic_descriptions2.csv\", index=False)\n","\n","# Add topic probabilities to the DataFrame\n","if probs is not None and isinstance(probs, np.ndarray) and probs.ndim == 2:\n","    topic_probs_df = pd.DataFrame(probs, columns=[f\"topic_{i}\" for i in range(probs.shape[1])])\n","    topic_probs_df.index = original_indices\n","    df = pd.concat([df, topic_probs_df], axis=1)\n","else:\n","    print(\"No topic probabilities found or the output format is unexpected.\")\n","\n","# Add the dominant topic for each document\n","df['Dominant_Topic'] = topics\n","\n","# Save the updated DataFrame with topic probabilities\n","df.to_csv(\"/content/drive/My Drive/optimized_merged_with_topics_and_probs2.csv\", index=False)\n","\n","# Save optimized BERTopic model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_new2\")\n","\n","# Plot results\n","plt.figure(figsize=(10, 5))\n","plt.plot(results_df[\"coherence_score\"], marker=\"o\", linestyle=\"-\", color=\"b\")\n","plt.xlabel(\"Model Iteration\")\n","plt.ylabel(\"Coherence Score\")\n","plt.title(\"Topic Coherence Across Different Parameter Settings\")\n","plt.show()\n"],"metadata":{"id":"OPmQk4vFk9fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ol3CcSGxvTr","executionInfo":{"status":"ok","timestamp":1741005610736,"user_tz":-60,"elapsed":2624,"user":{"displayName":"Darius R","userId":"06633547850250268555"}},"outputId":"b257c75c-4311-42bf-c0a4-beed884231fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"]}]},{"cell_type":"code","source":["#!pip install bertopic sentence-transformers umap-learn hdbscan\n","\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","from hdbscan import HDBSCAN\n","import umap\n","from gensim.models.coherencemodel import CoherenceModel\n","from gensim.corpora.dictionary import Dictionary\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/My Drive/mergeddf.csv\")\n","\n","# Ensure 'rawtext' column exists\n","if \"rawtext\" not in df.columns:\n","    raise ValueError(\"Column 'rawtext' not found in dataset.\")\n","\n","# Filter out NaN texts\n","df = df[df[\"rawtext\"].notna()].reset_index(drop=True)\n","texts = df[\"rawtext\"].tolist()\n","\n","# Load embedding model\n","embedding_model = SentenceTransformer(\"xlm-r-100langs-bert-base-nli-stsb-mean-tokens\")\n","\n","# Define parameter ranges to test\n","min_cluster_sizes = range(20, 50, 10)  # Adjusted to start at 10\n","n_neighbors_values = range(10, 50, 10)\n","n_components_values = [5, 7, 9]  # Increased upper bound\n","\n","# Store results\n","results = []\n","\n","# Iterate over all parameter combinations\n","for min_cluster_size in min_cluster_sizes:\n","    for n_neighbors in n_neighbors_values:\n","        for n_components in n_components_values:\n","            print(f\"Testing: min_cluster_size={min_cluster_size}, n_neighbors={n_neighbors}, n_components={n_components}\")\n","\n","            # Define UMAP and HDBSCAN models\n","            umap_model = umap.UMAP(n_neighbors=n_neighbors, n_components=n_components, metric='cosine')\n","            hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=5, prediction_data=True)  # ✅ Fix\n","\n","            # Initialize BERTopic\n","            topic_model = BERTopic(\n","                embedding_model=embedding_model,\n","                umap_model=umap_model,\n","                hdbscan_model=hdbscan_model,\n","                calculate_probabilities=True,\n","                verbose=True\n","            )\n","\n","            # Fit the model\n","            topics, _ = topic_model.fit_transform(texts)\n","\n","            # **Check if clustering worked**\n","            labels = topic_model.hdbscan_model.labels_\n","            if len(set(labels)) == 1 and -1 in set(labels):\n","                print(\"⚠ No valid clusters found! Skipping this model.\")\n","                continue  # Skip invalid models\n","\n","            # Compute Coherence Score using gensim\n","            topics_words = topic_model.get_topics()\n","            top_n_words = 10\n","            topic_list = [[word for word, _ in topic[:top_n_words]] for topic in topics_words.values()]\n","\n","            texts_tokenized = [text.split() for text in texts]\n","            dictionary = Dictionary(texts_tokenized)\n","            corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n","\n","            coherence_model = CoherenceModel(topics=topic_list, texts=texts_tokenized, dictionary=dictionary, coherence=\"c_v\")\n","            coherence = coherence_model.get_coherence()\n","\n","            # Save results\n","            results.append({\n","                \"min_cluster_size\": min_cluster_size,\n","                \"n_neighbors\": n_neighbors,\n","                \"n_components\": n_components,\n","                \"coherence_score\": coherence\n","            })\n","\n","# Convert results to DataFrame and save\n","results_df = pd.DataFrame(results)\n","results_df.to_csv(\"/content/drive/My Drive/coherence_scores.csv\", index=False)\n","\n","# Find best model\n","best_model = results_df.sort_values(by=\"coherence_score\", ascending=False).iloc[0]\n","best_min_cluster_size = best_model[\"min_cluster_size\"]\n","best_n_neighbors = best_model[\"n_neighbors\"]\n","best_n_components = best_model[\"n_components\"]\n","\n","print(f\"Best Parameters: min_cluster_size={best_min_cluster_size}, n_neighbors={best_n_neighbors}, n_components={best_n_components}\")\n","\n","# Re-run BERTopic with best parameters\n","optimal_umap = umap.UMAP(n_neighbors=best_n_neighbors, n_components=best_n_components, metric='cosine')\n","optimal_hdbscan = HDBSCAN(min_cluster_size=best_min_cluster_size, min_samples=5, prediction_data=True)\n","\n","topic_model = BERTopic(\n","    embedding_model=embedding_model,\n","    umap_model=optimal_umap,\n","    hdbscan_model=optimal_hdbscan,\n","    calculate_probabilities=True,\n","    verbose=True\n",")\n","\n","# Fit final model\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Save topic descriptions\n","topics_info = topic_model.get_topic_info()\n","topics_info.to_csv(\"/content/drive/My Drive/optimized_topic_descriptions.csv\", index=False)\n","\n","# Save optimized BERTopic model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model\")\n","\n","# Plot coherence scores\n","plt.figure(figsize=(10, 5))\n","plt.plot(results_df[\"coherence_score\"], marker=\"o\", linestyle=\"-\", color=\"b\", label=\"Coherence Score\")\n","plt.xlabel(\"Model Iteration\")\n","plt.ylabel(\"Coherence Score\")\n","plt.title(\"Topic Coherence Across Different Parameter Settings\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"u9aKSnMgx8o-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert results to DataFrame\n","results_df = pd.DataFrame(results)\n","\n","# Save coherence scores to CSV\n","results_df.to_csv(\"/content/drive/My Drive/coherence_scores.csv\", index=False)"],"metadata":{"id":"3F12sS9bzMlD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot coherence scores\n","plt.figure(figsize=(10, 5))\n","plt.plot(results_df[\"coherence_score\"], marker=\"o\", linestyle=\"-\", color=\"b\", label=\"Coherence Score\")\n","plt.xlabel(\"Model Iteration\")\n","plt.ylabel(\"Coherence Score\")\n","plt.title(\"Topic Coherence Across Different Parameter Settings\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"VqiygIjw0s9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","from hdbscan import HDBSCAN\n","import umap\n","from gensim.models.coherencemodel import CoherenceModel\n","from gensim.corpora.dictionary import Dictionary\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/My Drive/mergeddf.csv\")\n","\n","# Ensure 'rawtext' column exists\n","if \"rawtext\" not in df.columns:\n","    raise ValueError(\"Column 'rawtext' not found in dataset.\")\n","\n","# Filter out NaN texts\n","df = df[df[\"rawtext\"].notna()].reset_index(drop=True)\n","texts = df[\"rawtext\"].tolist()\n","\n","# Load embedding model\n","embedding_model = SentenceTransformer(\"xlm-r-100langs-bert-base-nli-stsb-mean-tokens\")\n","\n","# Define parameter ranges to extend search\n","min_cluster_sizes = [20]  # Keep 20 since it's the best found so far\n","n_neighbors_values = [40]  # Keep 30 as it was optimal\n","n_components_values = [11, 13]  # New values to test\n","\n","# Load existing results (if file exists)\n","results_file = \"/content/drive/My Drive/updated_coherence_scores.csv\"\n","try:\n","    existing_results_df = pd.read_csv(results_file)\n","    print(\"✅ Existing results loaded successfully!\")\n","except FileNotFoundError:\n","    print(\"⚠ No existing results found, starting fresh!\")\n","    existing_results_df = pd.DataFrame()  # Start fresh if no prior results\n","\n","# Store new results\n","new_results = []\n","\n","# Iterate over new n_components values\n","for n_components in n_components_values:\n","    print(f\"Testing: min_cluster_size=20, n_neighbors=30, n_components={n_components}\")\n","\n","    # Define UMAP and HDBSCAN models\n","    umap_model = umap.UMAP(n_neighbors=30, n_components=n_components, metric='cosine')\n","    hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=5, prediction_data=True)\n","\n","    # Initialize BERTopic\n","    topic_model = BERTopic(\n","        embedding_model=embedding_model,\n","        umap_model=umap_model,\n","        hdbscan_model=hdbscan_model,\n","        calculate_probabilities=True,\n","        verbose=True\n","    )\n","\n","    # Fit the model\n","    topics, _ = topic_model.fit_transform(texts)\n","\n","    # **Check if clustering worked**\n","    labels = topic_model.hdbscan_model.labels_\n","    if len(set(labels)) == 1 and -1 in set(labels):\n","        print(\"⚠ No valid clusters found! Skipping this model.\")\n","        continue  # Skip invalid models\n","\n","    # Compute Coherence Score using gensim\n","    topics_words = topic_model.get_topics()\n","    top_n_words = 10\n","    topic_list = [[word for word, _ in topic[:top_n_words]] for topic in topics_words.values()]\n","\n","    texts_tokenized = [text.split() for text in texts]\n","    dictionary = Dictionary(texts_tokenized)\n","    corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n","\n","    coherence_model = CoherenceModel(topics=topic_list, texts=texts_tokenized, dictionary=dictionary, coherence=\"c_v\")\n","    coherence = coherence_model.get_coherence()\n","\n","    # Save new results\n","    new_results.append({\n","        \"min_cluster_size\": 20,\n","        \"n_neighbors\": 40,\n","        \"n_components\": n_components,\n","        \"coherence_score\": coherence\n","    })\n","\n","# Convert new results to DataFrame\n","new_results_df = pd.DataFrame(new_results)\n","\n","# Append new results to existing results (without overwriting)\n","updated_results_df = pd.concat([existing_results_df, new_results_df], ignore_index=True)\n","\n","# Save updated results\n","updated_results_df.to_csv(results_file, index=False)\n","\n","print(\"✅ New results added and saved successfully!\")\n"],"metadata":{"id":"R2w0QQDx3JxC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot coherence scores\n","plt.figure(figsize=(10, 5))\n","plt.plot(updated_results_df[\"coherence_score\"], marker=\"o\", linestyle=\"-\", color=\"b\", label=\"Coherence Score\")\n","plt.xlabel(\"Model Iteration\")\n","plt.ylabel(\"Coherence Score\")\n","plt.title(\"Topic Coherence Across Different Parameter Settings\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"INhLtlRT6UPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize UMAP for broader, general topics\n","umap_model = umap.UMAP(n_neighbors=30, n_components=9, metric='cosine')\n","\n","# Initialize HDBSCAN to form fewer, larger clusters\n","hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=5)\n","\n","# Initialize BERTopic with custom UMAP and HDBSCAN models\n","topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True)\n","\n","# Fit the BERTopic model and save progress\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Extract document embeddings\n","embeddings = topic_model._extract_embeddings(texts, method=\"document\")\n","\n","# Get HDBSCAN labels\n","labels = topic_model.hdbscan_model.labels_\n","\n","# Calculate silhouette score\n","if len(set(labels)) > 1 and -1 not in set(labels):\n","    score = silhouette_score(embeddings, labels)\n","    print(f\"Silhouette Score: {score}\")\n","else:\n","    print(\"Not enough distinct clusters to compute silhouette score.\")\n","\n","# Display topic descriptions\n","topics_info = topic_model.get_topic_info()\n","print(\"\\nTopic Descriptions:\")\n","print(topics_info[['Topic', 'Name']])\n","\n","# Save the topic descriptions\n","topics_info.to_csv(\"/content/drive/My Drive/optimized_topic_descriptions5.csv\", index=False)\n","\n","# Add topic probabilities to the DataFrame\n","if probs is not None and isinstance(probs, np.ndarray) and probs.ndim == 2:\n","    topic_probs_df = pd.DataFrame(probs, columns=[f\"topic_{i}\" for i in range(probs.shape[1])])\n","    topic_probs_df.index = original_indices\n","    df = pd.concat([df, topic_probs_df], axis=1)\n","else:\n","    print(\"No topic probabilities found or the output format is unexpected.\")\n","\n","# Add the dominant topic for each document\n","df['Dominant_Topic'] = topics\n","\n","# Save the updated DataFrame with topic probabilities\n","df.to_csv(\"/content/drive/My Drive/optimized_merged_with_topics_and_probs5.csv\", index=False)\n","\n","# Save the optimized model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_general_final\")"],"metadata":{"id":"fhNK_6lRCCOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display topic descriptions with 30 words per topic\n","topics_info = topic_model.get_topic_info()\n","\n","# Extract 30 most distinctive words for each topic\n","num_words = 30\n","topics_info[\"Top_Words\"] = topics_info[\"Topic\"].apply(lambda x: \", \".join([word for word, _ in topic_model.get_topic(x)[:num_words]]))\n","\n","# Save the updated topic descriptions\n","topics_info.to_csv(\"/content/drive/My Drive/optimized_topic_descriptions_30words.csv\", index=False)\n","\n","print(\"\\nUpdated Topic Descriptions with 30 words:\")\n","print(topics_info[['Topic', 'Top_Words']])"],"metadata":{"id":"HA-GUmkPD7vF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import umap\n","from bertopic import BERTopic\n","from sentence_transformers import SentenceTransformer\n","from hdbscan import HDBSCAN\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/My Drive/mergeddf.csv\")\n","\n","# Ensure 'rawtext' column exists\n","if \"rawtext\" not in df.columns:\n","    raise ValueError(\"Column 'rawtext' not found in dataset.\")\n","\n","# Filter out NaN texts\n","df = df[df[\"rawtext\"].notna()].reset_index(drop=True)\n","texts = df[\"rawtext\"].tolist()\n","\n","# Load embedding model\n","embedding_model = SentenceTransformer(\"xlm-r-100langs-bert-base-nli-stsb-mean-tokens\")\n","\n","# Initialize UMAP\n","umap_model = umap.UMAP(n_neighbors=30, n_components=9, metric='cosine')\n","\n","# Initialize HDBSCAN with `prediction_data=True` for soft clustering\n","hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=5, prediction_data=True)\n","\n","# Initialize BERTopic with soft clustering\n","topic_model = BERTopic(\n","    embedding_model=embedding_model,\n","    umap_model=umap_model,\n","    hdbscan_model=hdbscan_model,\n","    calculate_probabilities=True,  # ✅ Enables soft clustering\n","    verbose=True\n",")\n","\n","# Fit the BERTopic model\n","topics, probs = topic_model.fit_transform(texts)\n","\n","# Assign each speech to the most probable topic (even if originally -1)\n","if probs is not None and isinstance(probs, np.ndarray) and probs.ndim == 2:\n","    max_prob_topics = probs.argmax(axis=1)  # Find the topic with the highest probability\n","    df[\"Dominant_Topic\"] = np.where(topics == -1, max_prob_topics, topics)  # Reassign noise speeches\n","else:\n","    df[\"Dominant_Topic\"] = topics  # If probs are not available, use direct topic assignment\n","\n","# Save topic probabilities for analysis\n","topic_probs_df = pd.DataFrame(probs, columns=[f\"topic_{i}\" for i in range(probs.shape[1])])\n","df = pd.concat([df, topic_probs_df], axis=1)\n","\n","# Save updated dataset\n","df.to_csv(\"/content/drive/My Drive/optimized_merged_with_topics_and_probs_soft.csv\", index=False)\n","\n","# Save the optimized BERTopic model\n","topic_model.save(\"/content/drive/My Drive/optimized_topic_model_general_soft\")\n","\n","# Extract 30 most distinctive words for each topic\n","num_words = 30\n","topics_info = topic_model.get_topic_info()\n","topics_info[\"Top_Words\"] = topics_info[\"Topic\"].apply(lambda x: \", \".join([word for word, _ in topic_model.get_topic(x)[:num_words]]))\n","\n","# Save topic descriptions with 30 words\n","topics_info.to_csv(\"/content/drive/My Drive/optimized_topic_descriptions_30words_soft.csv\", index=False)\n","\n","print(\"Soft clustering enabled, topics assigned, and dataset saved!\")\n"],"metadata":{"id":"NHvEKPx4F1Wz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topic_counts = df[\"Dominant_Topic\"].value_counts().reset_index()\n","topic_counts.columns = [\"Topic\", \"Speech_Count\"]\n","print(topic_counts)\n"],"metadata":{"id":"p4nWAX-iHN_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","# Load the dataset with topic probabilities\n","df = pd.read_csv(\"/content/drive/My Drive/optimized_merged_with_topics_and_probs_soft.csv\")\n","\n","# Identify the topic columns (assuming they are named topic_0, topic_1, ..., topic_63)\n","topic_cols = [col for col in df.columns if col.startswith(\"topic_\")]\n","\n","# Ensure probabilities exist\n","if len(topic_cols) == 0:\n","    raise ValueError(\"No topic probability columns found in the dataset.\")\n","\n","# Reassign noise (-1) speeches to the most probable topic\n","df[\"Reassigned_Topic\"] = df[\"Dominant_Topic\"]  # Start with the original topic assignments\n","df[\"Most_Probable_Topic\"] = df[topic_cols].idxmax(axis=1).str.replace(\"topic_\", \"\").astype(int)  # Find highest probability topic\n","\n","# Only reassign speeches that were originally labeled as -1\n","df[\"Reassigned_Topic\"] = np.where(df[\"Dominant_Topic\"] == -1, df[\"Most_Probable_Topic\"], df[\"Dominant_Topic\"])\n","\n","# Save the updated dataset with reassigned topics\n","df.to_csv(\"/content/drive/My Drive/optimized_merged_with_reassigned_topics.csv\", index=False)\n","\n","# Print summary\n","print(\"Noise speeches (-1) have been reassigned based on probability scores!\")\n","print(df[\"Reassigned_Topic\"].value_counts())  # Show how many speeches are in each topic now\n"],"metadata":{"id":"6DvOXM7WJNbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the path to the CSV file in Google Drive\n","csv_path = \"/content/drive/My Drive/coherence_scores.csv\"  # Update path if needed\n","\n","# Load the CSV file into a DataFrame\n","df = pd.read_csv(csv_path)\n","\n","# Debugging: Print column names to verify structure\n","print(\"Column names in dataset:\", df.columns)\n","\n","# Check if the necessary columns exist\n","expected_columns = [\"min_cluster_size\", \"n_neighbors\", \"n_components\", \"coherence_score\"]\n","if not all(col in df.columns for col in expected_columns):\n","    print(\"Some required columns are missing! Please check the dataset.\")\n","else:\n","    # Create a model specification label for plotting\n","    df['model_specification'] = df.apply(lambda row: f\"{row['min_cluster_size']}-{row['n_neighbors']}-{row['n_components']}\", axis=1)\n","\n","    # Check if coherence scores are too large and adjust if necessary\n","    if df['coherence_score'].max() > 1:  # Assuming coherence scores should be between 0 and 1\n","        df['coherence_score'] = df['coherence_score'] / 10**len(str(int(df['coherence_score'].max())))\n","\n","    # Plot the coherence scores\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(df['model_specification'], df['coherence_score'], marker='o', linestyle='-',\n","             color='#003399', markerfacecolor='#FFCC00', markeredgecolor='#003399', markersize=8)\n","\n","    # Improve plot aesthetics\n","    plt.xticks(rotation=45, ha='right', fontsize=10)\n","    plt.xlabel('Model Specification (min_cluster_size - n_neighbors - n_components)', fontsize=12)\n","    plt.ylabel('Coherence Score', fontsize=12)\n","    plt.title('Coherence Scores per Model Specification', fontsize=14,)\n","    plt.grid(True, linestyle='--', alpha=0.6)\n","    plt.tight_layout()\n","\n","    # Show the plot\n","    plt.show()\n"],"metadata":{"id":"BmNlu3XzNwlD"},"execution_count":null,"outputs":[]}]}